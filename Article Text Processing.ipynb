{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Goal of this is to clean up the documents via lemmatization, tokenization,  stop word removal, removal of pronouns, and removal of punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import pandas as pd\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "# if this doesn't work write python -m Spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty wordlist for finished product\n",
    "wordlist = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dataset from the previous notebook\n",
    "data = pd.read_csv('articleSummaryandPublishDate.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ArticleText</th>\n",
       "      <th>PublishDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Our live coverage of the coronavirus outbreak ...</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Please click here to read more live updates on...</td>\n",
       "      <td>2020-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Our live coverage of the Wuhan coronavirus out...</td>\n",
       "      <td>2020-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Our live coverage of the coronavirus outbreak ...</td>\n",
       "      <td>2020-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Our live coverage of the coronavirus outbreak ...</td>\n",
       "      <td>2020-02-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        ArticleText PublishDate\n",
       "0           0  Our live coverage of the coronavirus outbreak ...  2020-02-01\n",
       "1           1  Please click here to read more live updates on...  2020-02-02\n",
       "2           2  Our live coverage of the Wuhan coronavirus out...  2020-02-03\n",
       "3           3  Our live coverage of the coronavirus outbreak ...  2020-02-04\n",
       "4           4  Our live coverage of the coronavirus outbreak ...  2020-02-05"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "# The data we want is in the 'ArticleText' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable for final dataframe \n",
    "finalcolumnnames = ['ArticleText', 'PublishDate', 'CleanedArticleText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)\n",
    "# set finaldf variable for the end dataframe\n",
    "finaldf = pd.DataFrame(columns=finalcolumnnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    articletextvar = df.loc[i, \"ArticleText\"]\n",
    "    publishdatevar = df.loc[i, \"PublishDate\"]\n",
    "\n",
    "    dataAfterLemmaFilter = []\n",
    "    dataAfterPronounFilter = []\n",
    "    dataAfterStopwords = []\n",
    "    dataAfterPunctuations = []\n",
    "    dataAfterNounFilter = []\n",
    "    \n",
    "    # apply spacy model to the article text column\n",
    "    # to tokenize the document\n",
    "    doc = nlp(str(articletextvar))\n",
    "    \n",
    "    # lemmatize each token \n",
    "    for token in doc:\n",
    "        dataAfterLemmaFilter.append(token.lemma_)\n",
    "    \n",
    "    # remove pronouns\n",
    "    for token in dataAfterLemmaFilter:\n",
    "        if token != \"-PRON-\":\n",
    "            dataAfterPronounFilter.append(token.lower().strip())\n",
    "   \n",
    "    # remove stopwords from spacy's list of stop words\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    \n",
    "    # remove stop words \n",
    "    for token in dataAfterPronounFilter:\n",
    "        if token != stopwords:\n",
    "            dataAfterStopwords.append(token)    \n",
    "            \n",
    "    # custom list of words I noticed were in the dataset and wanted removed\n",
    "    custom_remove_list = ['our', 'live', 'coverage', 'of', 'the', 'has', 'moved', 'here', '\\\\n', 'http', ':', '//']\n",
    "    dataAfterCustomStopWords = []\n",
    "    for word in dataAfterStopwords:\n",
    "        if word not in custom_remove_list:\n",
    "            dataAfterCustomStopWords.append(word)\n",
    "            \n",
    "    # for some reason the word coverage stayed in my list, so I added another loop specifically to remove \n",
    "    # just that word\n",
    "    \n",
    "    dataAfterRemoveCoverage = []\n",
    "    # i used c because I am out of iterables\n",
    "    for c in dataAfterCustomStopWords:\n",
    "        if c != 'coverage':\n",
    "            dataAfterRemoveCoverage.append(c)\n",
    "            \n",
    "    # use 'string' library's punctuation list to remove all punctuation\n",
    "    punctuations = punctuation\n",
    "\n",
    "    for token in dataAfterRemoveCoverage:\n",
    "        if token not in punctuations:\n",
    "            dataAfterPunctuations.append(token)\n",
    "    \n",
    "    # remove noune using the 'en' model to find all the nouns\n",
    "    for value in dataAfterPunctuations:\n",
    "        td = nlp(value)\n",
    "        for t in td:\n",
    "            if t.pos_ == 'NOUN':\n",
    "                dataAfterNounFilter.append(t)\n",
    "                \n",
    "    # initialize empty list for all the tokens to be reassembled\n",
    "    dataAfterNounFilterStringFormatting = []\n",
    "    \n",
    "    # append each object to a string which is then appended to a list to be put into the dataframe\n",
    "    for nlpObject in dataAfterNounFilter:\n",
    "        nlpObjectIntoString=str(nlpObject)\n",
    "        dataAfterNounFilterStringFormatting.append(nlpObjectIntoString)\n",
    "    # rename for ease of use \n",
    "    cleaned_data_list = dataAfterNounFilterStringFormatting\n",
    "\n",
    "    # append the data to a temp dataframe 'df2' \n",
    "    df2 = pd.DataFrame({\"ArticleText\": [articletextvar],\n",
    "                        \"PublishDate\": [publishdatevar],\n",
    "                        \"CleanedArticleText\": [cleaned_data_list]})\n",
    "    # append the temp df to the final df \n",
    "    finaldf = finaldf.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleText</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>CleanedArticleText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our live coverage of the coronavirus outbreak ...</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>[end, today, people, mainland, end, country, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please click here to read more live updates on...</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>[update, field, departure, assistance, affairs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our live coverage of the Wuhan coronavirus out...</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>[hospital, statement, isolation, home, health,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our live coverage of the coronavirus outbreak ...</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>[health, people, death, toll, people, death, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our live coverage of the coronavirus outbreak ...</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>[number, case, increase, end, day, health, num...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ArticleText PublishDate  \\\n",
       "0  Our live coverage of the coronavirus outbreak ...  2020-02-01   \n",
       "1  Please click here to read more live updates on...  2020-02-02   \n",
       "2  Our live coverage of the Wuhan coronavirus out...  2020-02-03   \n",
       "3  Our live coverage of the coronavirus outbreak ...  2020-02-04   \n",
       "4  Our live coverage of the coronavirus outbreak ...  2020-02-05   \n",
       "\n",
       "                                  CleanedArticleText  \n",
       "0  [end, today, people, mainland, end, country, h...  \n",
       "1  [update, field, departure, assistance, affairs...  \n",
       "2  [hospital, statement, isolation, home, health,...  \n",
       "3  [health, people, death, toll, people, death, t...  \n",
       "4  [number, case, increase, end, day, health, num...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the results\n",
    "finaldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append dataframe to csv \n",
    "finaldf.to_csv('articleCleanedText.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The reason I did things the way I did was because I wanted to get alll the articles cleaned at once without having to separate this out into different notebooks. It was tough because I originally thought storing the data in a list inside of a dataframe would be smart, that way I could just extract it as a list for future use, but it turned out that's not the case, and it just made it so I have to clean it every time I need to use the list. I left it because it just shows the process of how to do it again if necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CoronavirusTopicModeling",
   "language": "python",
   "name": "coronavirustopicmodeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
